# eval_ragas.py
"""
RAGASë¡œ Advanced RAG íŒŒì´í”„ë¼ì¸ì„ í‰ê°€í•©ë‹ˆë‹¤.
Dataset í¬ë§·: JSONL (eval_dataset.jsonl)
ê° ì¤„ ì˜ˆì‹œ:
{"question": "ì €ìž‘ê¶Œ ì¹¨í•´ ì†í•´ë°°ìƒ ì‚°ì • ê¸°ì¤€ì€?",
 "gold_contexts": ["...ì •ë‹µ ê·¼ê±° ë¬¸ìž¥ A...", "...ê·¼ê±° ë¬¸ìž¥ B..."],  # ì„ íƒ(ì—†ì–´ë„ ì¼ë¶€ ì§€í‘œëŠ” ê°€ëŠ¥)
 "reference_answer": "íŒë¡€ìƒ ì†í•´ë°°ìƒ ì‚°ì •ì€ ..."                    # ì„ íƒ
}

ì‹¤í–‰:
(venv) > python eval_ragas.py --data eval_dataset.jsonl --limit 200
"""

import argparse, json
from pathlib import Path

from dotenv import load_dotenv
load_dotenv()

from rag_advanced import ask_advanced  # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ

# RAGAS
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)

def load_jsonl(path: str, limit: int | None = None):
    items = []
    with open(path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if limit and i >= limit:
                break
            line = line.strip()
            if not line:
                continue
            items.append(json.loads(line))
    return items

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data", type=str, default="eval_dataset.jsonl")
    ap.add_argument("--limit", type=int, default=None, help="í‰ê°€ ìƒ˜í”Œ ìƒí•œ")
    args = ap.parse_args()

    ds = load_jsonl(args.data, args.limit)
    if not ds:
        raise ValueError("ë°ì´í„°ì…‹ì´ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤. eval_dataset.jsonlì„ í™•ì¸í•˜ì„¸ìš”.")

    predictions, references = [], []

    for ex in ds:
        q = ex["question"]
        out = ask_advanced(q, history=ex.get("history", ""))  # ìš°ë¦¬ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ

        # ì˜ˆì¸¡: answer + ëª¨ë¸ì´ ì‚¬ìš©í•œ contexts (ì••ì¶•ë³¸ ê¸°ì¤€)
        ctxs = []
        for d in out.get("docs_compressed", []):
            ctxs.append(d.page_content)

        predictions.append({
            "question": q,
            "answer": out.get("answer", ""),
            "contexts": ctxs,
        })

        # ë ˆí¼ëŸ°ìŠ¤(ì„ íƒ í•­ëª© ìžˆì–´ë„/ì—†ì–´ë„ ë¨)
        references.append({
            "question": q,
            "contexts": ex.get("gold_contexts", []),     # ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸
            "answer": ex.get("reference_answer", ""),    # ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìžì—´
        })

    # RAGAS í‰ê°€ ì‹¤í–‰
    metrics = [faithfulness, answer_relevancy, context_precision, context_recall]
    result = evaluate(predictions=predictions, references=references, metrics=metrics)

    # ê²°ê³¼ ì¶œë ¥
    print("\n=== RAGAS Results (mean) ===")
    for k, v in result.items():
        # resultëŠ” dict-like; .get ë¡œ í‰ê· ê°’ ì ‘ê·¼ì´ ê°€ëŠ¥
        # ì¼ë¶€ ë²„ì „ì€ result[k].score ê°™ì€ ì†ì„±ì¼ ìˆ˜ ìžˆìœ¼ë‹ˆ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        try:
            score = getattr(v, "score", v)
        except Exception:
            score = v
        print(f"{k:20s}: {score:.4f}")

    # ìƒì„¸ ë¦¬í¬íŠ¸ ì €ìž¥
    out_path = Path("ragas_report.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, default=lambda o: getattr(o, "__dict__", str(o)))
    print(f"\nðŸ“„ Saved detailed report -> {out_path.resolve()}")

if __name__ == "__main__":
    main()
