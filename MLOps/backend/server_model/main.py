import os
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")

from contextlib import asynccontextmanager
import asyncio
import base64
from datetime import datetime
import importlib
import os
from pathlib import Path
from dotenv import load_dotenv

import pandas as pd
import pytz
from fastapi import FastAPI, APIRouter, File, UploadFile, HTTPException, Request, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, HTMLResponse, Response, JSONResponse
from . import model
from . import weight_used_model
# ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©, í˜„ì¬ í´ë”ì¸ servrer_model ìƒìœ„ í´ë”ì—ì„œ í˜„ ìœ„ì¹˜ ì¸ì‹
from fastapi.staticfiles import StaticFiles
# ì •ì  ë§ˆìš´íŠ¸ ì•„ë˜ì— ì¶”ê°€
from fastapi import Response

# uvicorn ì‹¤í–‰ ìœ„ì¹˜ì— ë”°ë¼ì„œ, íŒŒì¼ ê²½ë¡œ ì‹ë³„ì´ ë‹¬ë¼ì§€ëŠ” ì  í™•ì¸í•˜ê¸° (í˜„ì¬ ë””ë ‰í† ë¦¬ ìœ„ì¹˜ëŠ” model_servingì´ê³ , í•˜ìœ„ì— server_model ë””ë ‰í† ë¦¬ë‚´ì— main.pyê°€ ìˆë‹¤ê³  í•  ë•Œ)
# python -m uvicorn server_model.main:app --port 8001 --reload

# from . import config
# ì´ ê²½ìš°ëŠ” ìƒëŒ€ ê²½ë¡œë¡œì¨, í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ main.pyì™€ ê°™ì€ ë””ë ‰í† ë¦¬ ìœ„ì¹˜ì—ì„œ config.py ì°¾ì•„ì„œ ê°€ì ¸ì˜¤ë¯€ë¡œ, í•´ë‹¹ íŒŒì¼ í™•ì¸ í•„ìš”
# model_serving/server_model/config.py

from config import UPLOAD_DIR, IMAGE_DIR, MODEL_IMG_DIR
# ì´ ê²½ìš°ëŠ” í˜„ì¬ uvicorn ì‹¤í–‰í•œ ê²½ë¡œ ìœ„ì¹˜ì¸ model_servingê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ ìœ„ì¹˜ì—ì„œ config.py ì°¾ì•„ì„œ ê°€ì ¸ì˜¤ë¯€ë¡œ, í•´ë‹¹ íŒŒì¼ í™•ì¸ í•„ìš”
# model_serving/config.py

from .semiconductor_agent import SemiconductorRAGAgent

# -------------------------------------------------
# ê²½ë¡œ/ë””ë ‰í„°ë¦¬ ë° í”„ë¦¬í”½ìŠ¤(root_path)
# -------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent          # backend/server_model
SERVER_DIR = BASE_DIR.parent / "server"             # backend/server
DATA_DIR = BASE_DIR.parent / "artifacts/predictions"                        # backend/server_model/data
PUBLIC_DIR = BASE_DIR / "public"

UPLOAD_DIR = SERVER_DIR / "uploaded_files"
IMAGE_DIR = SERVER_DIR / "view-model-architecture"
MODEL_IMG_DIR = SERVER_DIR / "model-images"

# íƒ€ì„ì¡´
timezone = pytz.timezone("Asia/Seoul")
router = APIRouter()
load_dotenv()

# -------------------------------------------------
# Lifespan: ìŠ¤íƒ€íŠ¸ì—…ì„ ê°€ë³ê²Œ (ë¸”ë¡œí‚¹ ì‘ì—… ê¸ˆì§€)
# -------------------------------------------------
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì •ì /ê²°ê³¼ ë””ë ‰í„°ë¦¬ ë³´ì¥
    for d in (PUBLIC_DIR, UPLOAD_DIR, IMAGE_DIR, MODEL_IMG_DIR):
        Path(d).mkdir(parents=True, exist_ok=True)
    yield
    # ì¢…ë£Œ ì‹œ ë³„ë„ ì •ë¦¬ ì—†ìŒ

app = FastAPI(
    lifespan=lifespan,
    root_path="/",           # âœ… í”„ë¦¬í”½ìŠ¤ ë°˜ì˜
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://127.0.0.1:5173",
        "http://localhost:8001",
        "http://127.0.0.1:8001",
    ], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["Content-Disposition"],
)

# -------------------------------------------------
# ìœ í‹¸
# -------------------------------------------------
def _b64_png(path: Path) -> str:
    """PNG íŒŒì¼ì„ data URI(base64)ë¡œ ë³€í™˜"""
    if not path.exists():
        raise HTTPException(status_code=404, detail=f"Image not found: {path}")
    try:
        with open(path, "rb") as f:
            encoded = base64.b64encode(f.read()).decode("ascii")
        return "data:image/png;base64," + encoded
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reading image: {e}")

async def _read_csv_async(file_path: Path) -> pd.DataFrame:
    """CSVë¥¼ ìŠ¤ë ˆë“œì—ì„œ ì½ê¸° (ì´ë²¤íŠ¸ ë£¨í”„ ë¹„ë¸”ë¡œí‚¹)"""
    def _read():
        return pd.read_csv(file_path, index_col="Date", parse_dates=["Date"]).fillna("NaN")
    return await asyncio.to_thread(_read)

# main.py ë‚´ ë˜ëŠ” new_temperature_model.py ìƒë‹¨ì— ì¶”ê°€
async def _read_sensor_csv_async(file_path: Path) -> pd.DataFrame:
    """ì„¼ì„œ ì˜ˆì¸¡ìš© CSV (Timestamp, Value 2ì»¬ëŸ¼) ë¹„ë™ê¸° ë¡œë“œ"""
    def _read():
        df = pd.read_csv(file_path)
        # âœ… ì»¬ëŸ¼ ì´ë¦„ í™•ì¸
        expected_cols = ["timestamp", "Chamber_Temperature"]
        if len(df.columns) != 2:
            raise ValueError(f"Invalid CSV format: expected 2 columns, got {len(df.columns)}")
        if not all(col in df.columns for col in expected_cols):
            raise ValueError(f"CSV must contain columns: {expected_cols}")

        # âœ… Timestamp íŒŒì‹±
        df["Timestamp"] = pd.to_datetime(df["Timestamp"], errors="coerce")
        df = df.dropna(subset=["Timestamp"])

        # âœ… NaN ì²˜ë¦¬
        df = df.fillna(method="ffill").fillna(method="bfill")

        return df

    return await asyncio.to_thread(_read)

@router.get("/sensor-data")
async def get_sensor_data(tool: str):
    """
    íŠ¹ì • ì¥ë¹„(tool)ì˜ ì„¼ì„œ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜
    ì˜ˆ: /api/sensor-data?tool=Deposition
    """
    try:
        # tool ì´ë¦„ â†’ íŒŒì¼ëª… ë§¤í•‘
        file_map = {
            "Deposition": "Deposition_data.csv",
            "Etching": "Etching_data.csv",
            "Lithography": "Lithography_data.csv",
        }

        if tool not in file_map:
            raise HTTPException(status_code=400, detail=f"Invalid tool name: {tool}")

        file_path = DATA_DIR / file_map[tool]
        if not file_path.exists():
            raise HTTPException(status_code=404, detail=f"File not found: {file_path}")

        # CSV ì½ê¸°
        df = pd.read_csv(file_path)

        # Timestamp ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì •ë ¬
        if "Timestamp" in df.columns:
            df = df.sort_values("Timestamp")

        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ)
        keep_cols = [col for col in ["Timestamp", "Chamber_Temperature", "Gas_Flow_Rate", "Vacuum_Pressure"] if col in df.columns]
        df = df[keep_cols]

        # JSON ë³€í™˜
        data = df.to_dict(orient="records")
        return {"tool": tool, "data": data}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@router.post("/predict-temperature-json")
async def predict_temperature_json(file: UploadFile = File(...)):
    """
    ìƒˆë¡œìš´ ì˜¨ë„ ì˜ˆì¸¡ LSTM ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ê²°ê³¼(JSON)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        current_time = datetime.now(timezone).strftime("%Y%m%d_%H%M%S")
        new_filename = f"{current_time}_{file.filename}"
        file_location = Path(UPLOAD_DIR) / new_filename

        contents = await file.read()
        await asyncio.to_thread(file_location.write_bytes, contents)

        dataset = await _read_sensor_csv_async(file_location)

        # ìƒˆ ëª¨ë¸ ëª¨ë“ˆ ë¡œë“œ
        new_model_mod = importlib.import_module(".temperature_model", package=__package__)
        result_json = await asyncio.to_thread(new_model_mod.process_to_json, dataset)

        return result_json

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@router.get("/compare-sensor")
async def compare_sensor(sensor: str = "temperature", mode: str = "pred_half"):
    """
    ì„¼ì„œ íƒ€ì…ë³„ ì˜ˆì¸¡ vs ì‹¤ì œ ë°ì´í„° ë¹„êµ API
    mode: pred_half | real_half | real_full
    ì˜ˆ: /compare-sensor?sensor=temperature&mode=real_half
    """
    try:
        # âœ… ì„¼ì„œëª… â†’ íŒŒì¼ëª… ë§¤í•‘ (ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ)
        file_map = {
            "temperature": "De_chamber_temperature_prediction_vol1.csv",
            "gas": "De_gas_flow_rate_prediction_vol1.csv",
            "pressure": "De_rf_power_prediction_vol1.csv",
        }

        if sensor not in file_map:
            raise HTTPException(status_code=400, detail=f"Invalid sensor name: {sensor}")

        data_path = DATA_DIR / file_map[sensor]

        if not data_path.exists():
            raise HTTPException(status_code=404, detail=f"Data file not found: {data_path}")

        # âœ… CSV ë¡œë“œ ë° ì»¬ëŸ¼ ê²€ì¦
        df = pd.read_csv(data_path)
        df.columns = df.columns.str.strip()

        if not {"timestamp", "predicted", "actual"}.issubset(df.columns):
            raise HTTPException(status_code=400, detail="Missing required columns (timestamp, predicted, actual)")

        total_len = len(df)
        half_len = total_len // 2

        timestamps = df["timestamp"].tolist()
        predicted = df["predicted"].tolist()
        real = df["actual"].tolist()

        # âœ… modeë³„ ë™ì‘ ì²˜ë¦¬
        if mode == "pred_half":
            # ì˜ˆì¸¡ ì ˆë°˜ë§Œ
            timestamps = timestamps[:half_len]
            predicted = predicted[:half_len]
            real = [None] * half_len

        elif mode == "real_half":
            # ì „ì²´ ì˜ˆì¸¡ + ì‹¤ì œ ì ˆë°˜ (ë‚˜ë¨¸ì§€ None)
            real_half = real[:half_len]
            padding = [None] * (total_len - half_len)
            real = real_half + padding

        elif mode == "real_full":
            # ì „ì²´ ì˜ˆì¸¡ + ì „ì²´ ì‹¤ì œ
            pass  # ì´ë¯¸ full ë¦¬ìŠ¤íŠ¸ë¡œ ì¡´ì¬

        else:
            raise HTTPException(status_code=400, detail="Invalid mode parameter")

        # âœ… ì‘ë‹µ ë°˜í™˜
        return {
            "sensor": sensor,
            "timestamps": timestamps,
            "predicted": predicted,
            "real": real,
            "info": {
                "total_points": total_len,
                "half_points": half_len,
                "mode": mode,
            },
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/compare-sensor-updated")
async def compare_sensor_updated(sensor: str = "temperature"):
    """
    ëª¨ë¸ ì¬í•™ìŠµ í›„ ì˜ˆì¸¡ê°’(ì—…ë°ì´íŠ¸ CSV)ì„ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ë°˜í™˜
    - ê¸°ì¡´ ë°ì´í„°ëŠ” ìœ ì§€í•˜ê³ , ì´ API í˜¸ì¶œ ì‹œì—ë§Œ updated CSVë¥¼ ì‚¬ìš©
    """
    try:
        file_map = {
            "temperature": "De_chamber_temperature_prediction_vol1.csv",
            "gas": "De_gas_flow_rate_prediction_vol1.csv",
            "pressure": "De_rf_power_prediction_vol1.csv",
        }

        if sensor not in file_map:
            raise HTTPException(status_code=400, detail=f"Invalid sensor name: {sensor}")

        data_path = DATA_DIR / file_map[sensor]
        # âœ… ì›ë³¸ ë° ì—…ë°ì´íŠ¸ íŒŒì¼ ê²½ë¡œ
        base_path = data_path
        updated_path = data_path

        if not base_path.exists():
            raise HTTPException(status_code=404, detail=f"Base data not found: {base_path}")
        if not updated_path.exists():
            raise HTTPException(status_code=404, detail=f"Updated data not found: {updated_path}")

        df_base = pd.read_csv(base_path)
        df_new = pd.read_csv(updated_path)

        df_base.columns = df_base.columns.str.strip()
        df_new.columns = df_new.columns.str.strip()

        if not {"timestamp", "predicted", "actual"}.issubset(df_base.columns):
            raise HTTPException(status_code=400, detail="Base file missing required columns.")
        if "predicted" not in df_new.columns:
            raise HTTPException(status_code=400, detail="Updated file missing 'predicted' column.")

        total_len = len(df_base)
        half_len = total_len // 2

        # âœ… ê¸°ì¡´ ë°ì´í„° ë³µì‚¬ í›„ í›„ë°˜ë¶€ ì˜ˆì¸¡ê°’ë§Œ ì—…ë°ì´íŠ¸
        df_updated = df_base.copy()
        new_pred_values = df_new["predicted"].values[: total_len - half_len]
        df_updated.loc[half_len:, "predicted"] = new_pred_values

        # âœ… ê·¸ë˜í”„ ë°ì´í„° ìƒì„±
        timestamps = df_updated["timestamp"].tolist()
        predicted = df_updated["predicted"].tolist()
        real = df_updated["actual"].tolist() if "actual" in df_updated.columns else [None] * total_len

        return {
            "sensor": sensor,
            "timestamps": timestamps,
            "predicted": predicted,
            "real": real,
            "info": {
                "total_points": total_len,
                "replaced_points": len(new_pred_values),
                "message": "Updated predictions applied to back half of data"
            }
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@router.get("/update-retrained-prediction")
async def update_retrained_prediction(sensor: str = "temperature"):
    """
    ì¬í•™ìŠµëœ ì˜ˆì¸¡ CSV ê¸°ë°˜ìœ¼ë¡œ ê¸°ì¡´ ì˜ˆì¸¡ê°’ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” API.
    ì˜ˆ: /update-retrained-prediction?sensor=temperature
    """
    try:
        # âœ… ê¸°ì¡´ ë°ì´í„° (compare-sensorìš©)
        original_path = DATA_DIR / "De_chamber_temperature_matched.csv"
        retrained_path = DATA_DIR / "De_chamber_temperature_drift_prediction.csv"

        if not original_path.exists() or not retrained_path.exists():
            raise HTTPException(status_code=404, detail="Required data file not found.")

        df_orig = pd.read_csv(original_path)
        df_new = pd.read_csv(retrained_path)

        # âœ… ì»¬ëŸ¼ ì •ë¦¬
        df_orig.columns = df_orig.columns.str.strip()
        df_new.columns = df_new.columns.str.strip()

        if not {"Timestamp", "predicted"}.issubset(df_orig.columns) or not {"predicted"}.issubset(df_new.columns):
            raise HTTPException(status_code=400, detail="Missing required columns in either file.")

        total_len = len(df_orig)
        half_len = total_len // 2

        # âœ… í›„ë°˜ ì ˆë°˜ì„ ì¬í•™ìŠµ ì˜ˆì¸¡ê°’ìœ¼ë¡œ êµì²´
        df_updated = df_orig.copy()
        df_updated.loc[half_len:, "predicted"] = df_new["predicted"].values[: total_len - half_len]

        # âœ… ìƒˆë¡œìš´ CSV ì €ì¥
        updated_path = DATA_DIR / "De_chamber_temperature_updated.csv"
        df_updated.to_csv(updated_path, index=False)

        # âœ… ì‘ë‹µ ë°˜í™˜
        return {
            "status": "ok",
            "message": "Predicted values updated using retrained model output.",
            "updated_file": str(updated_path),
            "updated_rows": len(df_updated),
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
def _format_compare_payload(df: pd.DataFrame, sensor: str, mode: str = "real_full"):
    """
    /compare-sensor í˜•ì‹ìœ¼ë¡œ DataFrameì„ ì§ë ¬í™”í•´ ë°˜í™˜.
    dfì—ëŠ” ìµœì†Œ 'timestamp', 'predicted' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•˜ë©°,
    'actual'ì´ ì—†ìœ¼ë©´ realì€ None ë¦¬ìŠ¤íŠ¸ë¡œ ì±„ì›€.
    mode: pred_half | real_half | real_full
    """
    # ì»¬ëŸ¼ ì •ê·œí™”
    cols = {c.strip().lower(): c for c in df.columns}
    required_pred = "predicted"
    ts_key = "timestamp" if "timestamp" in cols else ("date" if "date" in cols else None)
    if ts_key is None:
        raise HTTPException(status_code=400, detail="CSV must contain a timestamp-like column (timestamp or date).")

    if required_pred not in cols:
        raise HTTPException(status_code=400, detail="CSV must contain 'predicted' column.")

    has_actual = "actual" in cols

    # ì •ë ¬(ê°€ëŠ¥ ì‹œ)
    try:
        # Timestampë¥¼ íŒŒì‹± ê°€ëŠ¥í•œ ê²½ìš° ì •ë ¬
        t = pd.to_datetime(df[cols[ts_key]], errors="coerce")
        df = df.assign(_ts=t).sort_values("_ts").drop(columns=["_ts"])
    except Exception:
        pass

    total_len = len(df)
    half_len = total_len // 2

    timestamps = df[cols[ts_key]].astype(str).tolist()
    predicted = df[cols[required_pred]].tolist()
    if has_actual:
        real = df[cols["actual"]].tolist()
    else:
        real = [None] * total_len

    # mode ì ìš©
    if mode == "pred_half":
        timestamps = timestamps[:half_len]
        predicted = predicted[:half_len]
        real = [None] * half_len
    elif mode == "real_half":
        real_half = real[:half_len]
        padding = [None] * (total_len - half_len)
        real = real_half + padding
    elif mode == "real_full":
        pass
    else:
        raise HTTPException(status_code=400, detail="Invalid mode parameter")

    return {
        "sensor": sensor,
        "timestamps": timestamps,
        "predicted": predicted,
        "real": real,
        "info": {
            "total_points": total_len,
            "half_points": half_len,
            "mode": mode,
        },
    }

@router.get("/predict-original-model")
async def predict_original_model(sensor: str = "temperature", mode: str = "real_full"):
    """
    ê¸°ì¡´(ì¬í•™ìŠµ ì´ì „) ëª¨ë¸ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , ê²°ê³¼ CSVë¥¼ ì €ì¥í•œ ë’¤
    /compare-sensorì™€ ë™ì¼ í˜•ì‹(sensor/timestamps/predicted/real/info)ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.
    mode: pred_half | real_half | real_full (ê¸°ë³¸ real_full)
    """
    try:
        import importlib
        inference_mod = importlib.import_module(".inference", package=__package__)

        model_map = {
            "temperature": {
                "model": "De_chamber_temperature_model.keras",
                "scaler": "De_chamber_temperature_scaler.pkl",
                "config": "De_chamber_temperature_config.json",
                "input_csv": "data/De_chamber_temperature.csv",
                "output_csv": "artifacts/predictions/De_chamber_temperature_prediction_vol1.csv",
            },
            "gas": {
                "model": "De_gas_flow_rate_model.keras",
                "scaler": "De_gas_flow_rate_scaler.pkl",
                "config": "De_gas_flow_rate_config.json",
                "input_csv": "data/De_gas_flow_rate.csv",
                "output_csv": "artifacts/predictions/De_gas_flow_rate_prediction_vol1.csv",
            },
            "pressure": {
                "model": "De_rf_power_model.keras",
                "scaler": "De_rf_power_scaler.pkl",
                "config": "De_rf_power_config.json",
                "input_csv": "data/De_rf_power.csv",
                "output_csv": "artifacts/predictions/De_rf_power_prediction_vol1.csv",
            },
        }

        if sensor not in model_map:
            raise HTTPException(status_code=400, detail=f"Invalid sensor name: {sensor}")

        base_dir = Path(__file__).resolve().parent.parent
        artifacts_dir = base_dir / "artifacts"
        data_dir = base_dir / "server_model" / "data"

        m = model_map[sensor]
        model_path = artifacts_dir / "model" / m["model"]
        scaler_path = artifacts_dir / "scaler" / m["scaler"]
        config_path = artifacts_dir / "config" / m["config"]
        input_csv = Path(m["input_csv"])
        output_csv = Path(m["output_csv"])

        if not model_path.exists():
            raise HTTPException(status_code=404, detail=f"Model file not found: {model_path}")
        if not scaler_path.exists():
            raise HTTPException(status_code=404, detail=f"Scaler file not found: {scaler_path}")
        if not config_path.exists():
            raise HTTPException(status_code=404, detail=f"Config file not found: {config_path}")
        if not input_csv.exists():
            raise HTTPException(status_code=404, detail=f"Input CSV not found: {input_csv}")

        # ì˜ˆì¸¡ ìˆ˜í–‰ (ë™ê¸° â†’ ìŠ¤ë ˆë“œ)
        predictor = inference_mod.TemperaturePredictionModel(
            model_path=str(model_path),
            scaler_path=str(scaler_path),
            config_path=str(config_path)
        )
        result_df = await asyncio.to_thread(
            predictor.predict_from_csv,
            str(input_csv),
            str(output_csv)
        )

        # ê²°ê³¼ CSV ë³´ì¥
        if not output_csv.exists():
            raise HTTPException(status_code=500, detail=f"Output CSV not created: {output_csv}")

        # ë°©ê¸ˆ ìƒì„±í•œ CSVë¥¼ ë¡œë“œí•˜ì—¬ /compare-sensor í¬ë§·ìœ¼ë¡œ ì§ë ¬í™”
        df_out = pd.read_csv(output_csv)
        # ì»¬ëŸ¼ ì •ê·œí™”
        df_out.columns = df_out.columns.str.strip()

        # ì¼ë¶€ íŒŒì´í”„ë¼ì¸ì€ ì»¬ëŸ¼ëª…ì´ 'Timestamp'ë¡œ ì¶œë ¥ë  ìˆ˜ ìˆìœ¼ë‹ˆ ë³´ì™„
        # ë˜í•œ 'actual'ì´ ì—†ì„ ìˆ˜ ìˆìŒ(ê·¸ ê²½ìš° realì€ Noneìœ¼ë¡œ ì±„ì›€)
        payload = _format_compare_payload(df_out, sensor=sensor, mode=mode)

        # í•„ìš”í•˜ë©´ ìƒíƒœ/ë©”ì‹œì§€ ë“± ë©”íƒ€ë„ í•¨ê»˜ ë°˜í™˜í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ì²˜ëŸ¼ ë³‘í•© ê°€ëŠ¥
        payload["status"] = "ok"
        payload["message"] = f"Original model inference complete for '{sensor}'"
        payload["csv_path"] = str(output_csv)

        return payload

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

from fastapi.encoders import jsonable_encoder

MAX_JSON_ROWS_PER_SENSOR = 0  # 0 ë˜ëŠ” Noneì´ë©´ ì „ì²´ í–‰ ì „ì†¡

def _read_csv_as_json(path: str, max_rows: int | None = MAX_JSON_ROWS_PER_SENSOR):
    import pandas as pd, numpy as np
    from pathlib import Path
    p = Path(path)
    if not p.exists():
        return {"error": f"CSV not found: {path}", "rows": 0, "records": []}
    df = pd.read_csv(p)
    # Timestamp â†’ ë¬¸ìì—´ ì •ê·œí™”(ìˆìœ¼ë©´)
    if "Timestamp" in df.columns:
        ts = pd.to_datetime(df["Timestamp"], errors="coerce")
        df["Timestamp"] = ts.dt.strftime("%Y-%m-%d %H:%M:%S").fillna(df["Timestamp"].astype(str))
    if max_rows and max_rows > 0:
        df = df.head(max_rows)
    df = df.replace({np.nan: None})
    return {"rows": len(df), "records": df.to_dict(orient="records")}
    
@router.post("/retrain")
async def retrain_models():
    try:
        import importlib, asyncio, pandas as pd
        retrain_mod = importlib.import_module(".retrain", package=__package__)
        sensors = retrain_mod.SENSORS
        results = []
        for s in sensors:
            res = await asyncio.to_thread(
                retrain_mod.retrain_one_sensor,
                s["csv_path"], s["column"], s["artifacts_prefix"]
            )
            results.append({
                "sensor": s["key"],
                "version": res.get("version"),
                "csv": str(res.get("csv")),
                "rows_csv": res.get("rows_csv", None),
            })

        versions = [r["version"] for r in results if r.get("version")]
        last_version = max([int(v.replace("vol","")) for v in versions], default=1)

        summary_name = f"retrain_summary_vol{last_version}.csv"
        summary_path = retrain_mod.ARTIFACTS_DIR / summary_name
        pd.DataFrame(results).to_csv(summary_path, index=False)

        # âœ… ì„¼ì„œë³„ CSV â†’ JSON ë¶™ì´ê¸°
        data_by_sensor = {}
        for r in results:
            data_by_sensor[r["sensor"]] = _read_csv_as_json(r["csv"])

        payload = {
            "status": "ok",
            "message": f"All sensors retrained (version {last_version}) successfully.",
            "results": results,
            "summary_csv": str(summary_path),
            "data": data_by_sensor,  # â† í”„ë¡ íŠ¸ê°€ í•œ ë²ˆì— ê°€ì ¸ê°
        }
        return jsonable_encoder(payload)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/generate-report")
async def generate_report():
    """
    LLM ê¸°ë°˜ ë°˜ë„ì²´ ì˜ˆì¸¡ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± API
    """
    try:
        data_dir = Path(r"D:\skala_workspace\MLOps\backend\data")
        data = sorted(str(p) for p in data_dir.glob("*.pdf"))
        print("[DEBUG] Knowledge PDFs:", data)
        agent = SemiconductorRAGAgent(knowledge_pdf_paths=data, force_reembed=True)
        agent.run_full_analysis(
            data_file_path="semiconductor_quality_control.csv",
            output_pdf_path="ë°˜ë„ì²´_ë¶„ì„_ë³´ê³ ì„œ4.pdf"
        )
        resp = FileResponse(
            "ë°˜ë„ì²´_ë¶„ì„_ë³´ê³ ì„œ4.pdf",
            media_type="application/pdf",
            filename="analysis_report.pdf",
        )
        # ğŸ‘‡ ìºì‹œ ë°©ì§€
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        return resp

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
from typing import List

@router.post("/upload-and-reembed")
async def kb_upload_and_reembed(files: List[UploadFile] = File(...)):
    """
    ì—¬ëŸ¬ ê°œ PDFë¥¼ í•œ ë²ˆì— ì—…ë¡œë“œí•˜ê³ , update í´ë”ì— ì €ì¥í•œ ë’¤
    add_new_documents()ë¡œ ë²¡í„°DBì— ì¦‰ì‹œ ì¶”ê°€(ì¬ì„ë² ë”©)í•©ë‹ˆë‹¤.
    """
    try:
        data_dir   = Path(r"D:\skala_workspace\MLOps\backend\data")
        update_dir = Path(r"D:\skala_workspace\MLOps\backend\update")
        update_dir.mkdir(parents=True, exist_ok=True)

        if not files:
            raise HTTPException(status_code=400, detail="ì—…ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        saved_paths: list[str] = []
        errors: list[dict] = []

        # 1) ì—…ë¡œë“œ ì €ì¥ (ì—¬ëŸ¬ ê°œ)
        for f in files:
            try:
                ext = Path(f.filename).suffix.lower()
                if ext != ".pdf":
                    raise ValueError(f"PDFë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤: {f.filename}")

                ts_name = datetime.now(timezone).strftime("%Y%m%d_%H%M%S_") + f.filename
                dest = update_dir / ts_name

                content = await f.read()
                await asyncio.to_thread(dest.write_bytes, content)

                saved_paths.append(str(dest))
            except Exception as e:
                errors.append({"file": f.filename, "error": str(e)})

        if not saved_paths and errors:
            # ëª¨ë‘ ì‹¤íŒ¨í•œ ê²½ìš°
            raise HTTPException(status_code=400, detail={"message":"ëª¨ë“  íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨", "errors": errors})

        # 2) ì—ì´ì „íŠ¸ ìƒì„±(ê¸°ì¡´ KBëŠ” ìœ ì§€, ê°•ì œ ì¬ì„ë² ë”© X)
        base_kb = sorted(p.name for p in data_dir.glob("*.pdf"))  # ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ name ì‚¬ìš©
        print("[DEBUG] KB base PDFs:", base_kb, flush=True)

        agent = SemiconductorRAGAgent(
            knowledge_pdf_paths=base_kb,
            force_reembed=False,  # ê¸°ì¡´ ì„ë² ë”© ì¬ì‚¬ìš©
        )

        # 3) ìƒˆ ë¬¸ì„œë“¤ ì¼ê´„ ì¶”ê°€ ì„ë² ë”© (ë¸”ë¡œí‚¹ ë°©ì§€)
        print("[DEBUG] Add new docs:", saved_paths, flush=True)
        await asyncio.to_thread(agent.add_new_documents, saved_paths)

        # 4) ê²°ê³¼ ë¦¬í„´
        payload = {
            "status": "ok",
            "message": f"{len(saved_paths)}ê°œ ë¬¸ì„œë¥¼ ì¶”ê°€í–ˆê³  ë²¡í„°DBì— ë°˜ì˜í–ˆìŠµë‹ˆë‹¤.",
            "added_files": saved_paths,
        }
        if errors:
            payload["partial_errors"] = errors
        return payload

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

app.include_router(router)

# ì‹¤í–‰ ëª…ë ¹ì–´ ì˜ˆì‹œ: ìˆœì„œëŒ€ë¡œ ë°±ì—”ë“œ ë„ìš´ í›„, í”„ë¡ íŠ¸ì—”ë“œ ë„ìš°ê¸°, í˜„ì¬ ë””ë ‰í† ë¦¬ server_model ìƒìœ„ì—ì„œ ì‹¤í–‰ (ìƒëŒ€ ê²½ë¡œ . ì‚¬ìš©)
# python -m uvicorn server_model.main:app --port 8001 --reload
# http://localhost:8001/static/index.html
