# -*- coding: utf-8 -*-
import os, json, math, pickle, warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

# =============================
# ê²½ë¡œ/í•˜ë“œì½”ë”©
# =============================
BASE_DIR  = "saved_model"      # â† ì´ì „(ê¸°ì¤€) ëª¨ë¸ ì„¸íŠ¸ ìœ„ì¹˜ (.keras/.pkl/.json)
MODEL_DIR = "re_saved_model"   # â† ì¬í•™ìŠµ ì‚°ì¶œë¬¼ ì €ì¥ ìœ„ì¹˜

# ê¸°ë³¸ê°’ (base ëª¨ë¸ì´ ìˆìœ¼ë©´ baseì˜ seq_lenì„ ë”°ë¦„)
DEFAULT_SEQ_LEN = 8
DENSIFY_FACTOR  = 10  # ë³´ê°„ ë°°ìœ¨(5~20 ê¶Œì¥)
EPOCHS_PRETRAIN = 120
EPOCHS_FINETUNE = 40
BATCH_SIZE      = 64
LR_PRETRAIN     = 1e-3
LR_FINETUNE     = 1e-4

# ë¶„í•  ë¹„ìœ¨: 75%(pretrain) / 20%(finetune) / 5%(holdout)
SPLIT_PRE = 0.75
SPLIT_FT  = 0.95

SENSORS = [
    {
        "key": "chamber_temperature",
        "csv_path": "drift_prediction_results_vol2/De_chamber_temperature_drift_prediction.csv",
        "column": "actual",
        "artifacts_prefix": "De_chamber_temperature",
    },
    {
        "key": "gas_flow_rate",
        "csv_path": "drift_prediction_results_vol2/De_gas_flow_rate_drift_prediction.csv",
        "column": "actual",
        "artifacts_prefix": "De_gas_flow_rate",
    },
    {
        "key": "rf_power",
        "csv_path": "drift_prediction_results_vol2/De_rf_power_drift_prediction.csv",
        "column": "actual",
        "artifacts_prefix": "De_rf_power",
    },
]

# =============================
# ìœ í‹¸
# =============================
def rmse(y_true, y_pred):
    return math.sqrt(mean_squared_error(y_true, y_pred))

def mape(y_true, y_pred, eps=1e-8):
    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)
    mask = np.abs(y_true) > eps
    if mask.sum() == 0: return 0.0
    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / (y_true[mask] + eps))) * 100.0)

def make_supervised(series: np.ndarray, seq_len: int):
    """ë‹¨ì¼(ìŠ¤ì¼€ì¼ëœ) ì‹œê³„ì—´ â†’ (X,y) ê°ë…í•™ìŠµ ì„¸íŠ¸"""
    if len(series) <= seq_len:
        return np.zeros((0, seq_len, 1), dtype=np.float32), np.zeros((0, 1), dtype=np.float32)
    X, y = [], []
    for i in range(len(series) - seq_len):
        X.append(series[i:i+seq_len])
        y.append(series[i+seq_len])
    n = len(X)
    X = np.array(X, dtype=np.float32).reshape(n, seq_len, 1)
    y = np.array(y, dtype=np.float32).reshape(n, 1)
    return X, y

def build_lstm(seq_len: int, hidden: int = 96, dropout: float = 0.25, lr: float = 1e-3) -> keras.Model:
    """ìƒˆ ëª¨ë¸ì„ ë§Œë“¤ì–´ì•¼ í•  ë•Œë§Œ ì‚¬ìš©(ë² ì´ìŠ¤ê°€ ì—†ì„ ë•Œ)"""
    inp = layers.Input(shape=(seq_len, 1))
    x = layers.LSTM(hidden, return_sequences=False)(inp)
    x = layers.Dropout(dropout)(x)
    x = layers.Dense(hidden//2, activation="relu")(x)
    out = layers.Dense(1)(x)
    model = keras.Model(inp, out)
    model.compile(optimizer=keras.optimizers.Adam(lr), loss="mse")
    return model

def detect_time_column(df: pd.DataFrame):
    for c in ["timestamp", "time", "datetime", "date"]:
        if c in df.columns:
            return c
    return None

def densify_with_interpolation(values: np.ndarray, timestamps: pd.Series | None, factor: int):
    """
    ì„ í˜• ë³´ê°„ìœ¼ë¡œ factorë°° í™•ì¥(í•™ìŠµìš© êµ¬ê°„ë§Œ).
    timestampê°€ ìˆìœ¼ë©´ median ê°„ê²© ê¸°ë°˜ ë¦¬ìƒ˜í”Œ í›„ time-interp, ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ê¸°ë°˜ interp.
    """
    if factor <= 1 or len(values) < 2:
        return values.copy(), (pd.to_datetime(timestamps).values if timestamps is not None else None)

    if timestamps is not None:
        ts = pd.to_datetime(timestamps)
        if len(ts) < 2:
            return values.copy(), ts.values
        deltas = (ts[1:].values.astype("datetime64[ns]") - ts[:-1].values.astype("datetime64[ns]")).astype("timedelta64[ns]").astype(np.int64)
        base_ns = int(np.median(deltas)) if len(deltas) > 0 else 0
        if base_ns <= 0:
            # fallback: ì¸ë±ìŠ¤ ë³´ê°„
            x = np.arange(len(values))
            x_new = np.linspace(0, len(values)-1, num=(len(values)-1)*factor+1)
            v_new = np.interp(x_new, x, values)
            return v_new.astype(float), None
        fine_ns = max(1, base_ns // factor)
        new_index = pd.date_range(start=ts.iloc[0], end=ts.iloc[-1], freq=pd.to_timedelta(fine_ns, unit="ns"))
        df_tmp = pd.DataFrame({"v": values}, index=ts)
        df_res = df_tmp.reindex(df_tmp.index.union(new_index)).sort_index().interpolate(method="time").reindex(new_index)
        return df_res["v"].values.astype(float), new_index.values
    else:
        x = np.arange(len(values))
        x_new = np.linspace(0, len(values)-1, num=(len(values)-1)*factor+1)
        v_new = np.interp(x_new, x, values)
        return v_new.astype(float), None

def split_indices(n: int, pre_ratio: float, ft_ratio: float):
    """ì´ ê¸¸ì´ n â†’ [0:pre) pretrain / [pre:ft) finetune / [ft:n) holdout"""
    pre_idx = max(1, int(n * pre_ratio))
    ft_idx  = max(pre_idx + 1, int(n * ft_ratio))
    ft_idx  = min(ft_idx, n - 1)  # holdout ìµœì†Œ 1í¬ì¸íŠ¸ ë³´ì¥
    return pre_idx, ft_idx

def base_artifacts(prefix: str):
    """saved_model ê²½ë¡œì˜ ë² ì´ìŠ¤ ì•„í‹°íŒ©íŠ¸ ê²½ë¡œ ì‚¬ì „"""
    return {
        "model":  os.path.join(BASE_DIR,  f"{prefix}_model.keras"),
        "scaler": os.path.join(BASE_DIR,  f"{prefix}_scaler.pkl"),
        "config": os.path.join(BASE_DIR,  f"{prefix}_config.json"),
    }

def load_base_if_exists(prefix: str):
    """ë² ì´ìŠ¤(.keras/.pkl/.json)ë¥¼ ë¡œë“œ. ì—†ìœ¼ë©´(None, None, None) ë°˜í™˜."""
    paths = base_artifacts(prefix)
    if not all(os.path.exists(p) for p in paths.values()):
        return None, None, None
    base_model = tf.keras.models.load_model(paths["model"])
    with open(paths["scaler"], "rb") as f:
        base_scaler = pickle.load(f)
    with open(paths["config"], "r", encoding="utf-8") as f:
        base_config = json.load(f)
    return base_model, base_scaler, base_config

# =============================
# ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸ (í•œ ì„¼ì„œ, ë² ì´ìŠ¤ warm-start)
# =============================
def retrain_one_sensor(csv_path: str, value_col: str, artifacts_prefix: str):
    os.makedirs(MODEL_DIR, exist_ok=True)

    print("\n" + "="*90)
    print(f"[{artifacts_prefix}] ì¬í•™ìŠµ ì‹œì‘ (warm-start from saved_model if available)")
    print("="*90)

    # 0) ë² ì´ìŠ¤ ë¡œë“œ ì‹œë„
    base_model, base_scaler, base_config = load_base_if_exists(artifacts_prefix)
    if base_model is not None:
        # ë² ì´ìŠ¤ ëª¨ë¸ ì…ë ¥ ê¸¸ì´ ê°ì§€
        input_shape = base_model.inputs[0].shape  # (None, seq_len, 1)
        base_seq_len = int(input_shape[1])
        seq_len = base_seq_len
        print(f"[{artifacts_prefix}] âœ… Base model found. Using base seq_len={seq_len}")
    else:
        seq_len = DEFAULT_SEQ_LEN
        print(f"[{artifacts_prefix}] âš ï¸ Base model not found. Train from scratch with seq_len={seq_len}")

    # 1) ë°ì´í„° ë¡œë“œ/ì •ë ¬
    df = pd.read_csv(csv_path)
    if value_col not in df.columns:
        raise ValueError(f"[{artifacts_prefix}] '{value_col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. CSV ì»¬ëŸ¼: {list(df.columns)}")
    time_col = detect_time_column(df)
    if time_col is not None:
        df[time_col] = pd.to_datetime(df[time_col])
        df = df.sort_values(time_col).reset_index(drop=True)

    series_raw = df[value_col].astype(float).values
    n_all = len(series_raw)
    print(f"[{artifacts_prefix}] ì›ì‹œ í¬ì¸íŠ¸: {n_all}, ë²”ìœ„: {np.min(series_raw):.4f} ~ {np.max(series_raw):.4f}")
    if n_all < seq_len + 10:
        print(f"[{artifacts_prefix}] âš ï¸ ë°ì´í„°ê°€ ë§¤ìš° ì§§ìŠµë‹ˆë‹¤. (n={n_all}) â€” ë³´ê°„ í™•ëŒ€ ì‚¬ìš© ê¶Œì¥")

    # 2) 75/95 ë¶„í• 
    pre_idx, ft_idx = split_indices(n_all, SPLIT_PRE, SPLIT_FT)
    pre_raw = series_raw[:pre_idx]
    ft_raw  = series_raw[pre_idx:ft_idx]
    val_raw = series_raw[ft_idx:]

    ts_series = df[time_col] if time_col is not None else None
    pre_ts = ts_series.iloc[:pre_idx] if ts_series is not None else None
    ft_ts  = ts_series.iloc[pre_idx:ft_idx] if ts_series is not None else None
    val_ts = ts_series.iloc[ft_idx:] if ts_series is not None else None

    print(f"[{artifacts_prefix}] ë¶„í•  â†’ pre:{len(pre_raw)}  finetune:{len(ft_raw)}  val(holdout):{len(val_raw)}")

    # 3) ë³´ê°„(í•™ìŠµìš©ë§Œ)
    pre_dense, pre_dense_ts = densify_with_interpolation(pre_raw, pre_ts, DENSIFY_FACTOR)
    ft_dense,  ft_dense_ts  = densify_with_interpolation(ft_raw,  ft_ts,  DENSIFY_FACTOR)
    print(f"[{artifacts_prefix}] ë³´ê°„ â†’ pre:{len(pre_dense)}  finetune:{len(ft_dense)}  (val ì›ì‹œ:{len(val_raw)})")

    # 4) ìŠ¤ì¼€ì¼ëŸ¬: ìƒˆë¡œ fit (ê¶Œì¥) â€” ë“œë¦¬í”„íŠ¸ ì ì‘ì„ ìœ„í•´ í•™ìŠµ ì§‘í•© ê¸°ì¤€
    #    (ì •ë§ í•„ìš”í•˜ë©´ base_scalerë¥¼ ê·¸ëŒ€ë¡œ ì¨ë„ ë˜ì§€ë§Œ, ë¶„í¬ê°€ ë‹¬ë¼ì¡Œë‹¤ë©´ ì„±ëŠ¥ ì €í•˜ ìœ„í—˜)
    scaler = MinMaxScaler()
    train_all = np.concatenate([pre_dense, ft_dense]) if len(ft_dense) > 0 else pre_dense
    scaler.fit(train_all.reshape(-1, 1))

    pre_sc = scaler.transform(pre_dense.reshape(-1, 1)).reshape(-1)
    ft_sc  = scaler.transform(ft_dense.reshape(-1, 1)).reshape(-1)
    val_sc = scaler.transform(val_raw.reshape(-1, 1)).reshape(-1)

    # 5) ê°ë…í•™ìŠµ ë°ì´í„°
    X_pre, y_pre = make_supervised(pre_sc, seq_len)
    X_ft,  y_ft  = make_supervised(ft_sc,  seq_len)

    print(f"[{artifacts_prefix}] supervised â†’ X_pre:{X_pre.shape}  X_ft:{X_ft.shape}")

    # 6) ëª¨ë¸: baseê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©(ê°€ì¤‘ì¹˜ í¬í•¨), ì—†ìœ¼ë©´ ìƒˆë¡œ ë¹Œë“œ
    if base_model is not None:
        model = base_model
        # í•™ìŠµë¥  ì„¤ì • í›„ compile
        model.compile(optimizer=keras.optimizers.Adam(learning_rate=float(LR_PRETRAIN)), loss="mse")
        print(f"[{artifacts_prefix}] â–¶ Warm-start pretrain ...")
    else:
        model = build_lstm(seq_len=seq_len, hidden=96, dropout=0.25, lr=LR_PRETRAIN)
        print(f"[{artifacts_prefix}] â–¶ Scratch pretrain ...")

    # Pretrain (ëê¹Œì§€)
    cbs_pre = [keras.callbacks.ReduceLROnPlateau(monitor="loss", factor=0.5, patience=4, min_lr=1e-5)]
    model.fit(
        X_pre, y_pre,
        epochs=EPOCHS_PRETRAIN,
        batch_size=BATCH_SIZE,
        shuffle=False,
        verbose=2,
        callbacks=cbs_pre,
    )

    # Finetune (ìµœê·¼ 20% ì ì‘; í•™ìŠµë¥  ë‚®ì¶° ê³„ì†)
    if X_ft.shape[0] > 0:
        model.compile(optimizer=keras.optimizers.Adam(learning_rate=float(LR_FINETUNE)), loss="mse")
        cbs_ft = [keras.callbacks.ReduceLROnPlateau(monitor="loss", factor=0.5, patience=3, min_lr=1e-5)]
        print(f"[{artifacts_prefix}] â–¶ Finetune ...")
        model.fit(
            X_ft, y_ft,
            epochs=EPOCHS_FINETUNE,
            batch_size=BATCH_SIZE,
            shuffle=False,
            verbose=2,
            callbacks=cbs_ft,
        )
    else:
        print(f"[{artifacts_prefix}] âš ï¸ Finetune êµ¬ê°„ì´ ë¹„ì–´ ê±´ë„ˆëœ€")

    # 7) ì „ì²´ êµ¬ê°„ 1-step ì˜ˆì¸¡ ìƒì„± (CSV ì €ì¥ìš©)
    # full_values = pre_dense + ft_dense + val_raw (ft_denseê°€ ì—†ìœ¼ë©´ pre_dense + val_raw)
    full_values = np.concatenate([pre_dense, ft_dense, val_raw]) if len(ft_dense) > 0 else np.concatenate([pre_dense, val_raw])
    full_scaled = scaler.transform(full_values.reshape(-1, 1)).reshape(-1)
    X_full, y_full = make_supervised(full_scaled, seq_len)
    if X_full.shape[0] == 0:
        raise ValueError(f"[{artifacts_prefix}] ì „ì²´ êµ¬ê°„ì´ ë„ˆë¬´ ì§§ì•„ ì˜ˆì¸¡ ìƒì„± ë¶ˆê°€")
    y_full_hat = model.predict(X_full, verbose=0).reshape(-1)

    # ì—­ë³€í™˜
    y_full_inv     = scaler.inverse_transform(y_full.reshape(-1,1)).flatten()
    y_full_hat_inv = scaler.inverse_transform(y_full_hat.reshape(-1,1)).flatten()

    # timestamp & input_data ì •ë ¬
    def _mk_ts(ts):
        if ts is None: return None
        return pd.to_datetime(ts)

    pre_dense_ts = _mk_ts(pre_dense_ts)
    ft_dense_ts  = _mk_ts(ft_dense_ts)
    val_ts       = _mk_ts(val_ts)

    if pre_dense_ts is None and ft_dense_ts is None and val_ts is None:
        full_ts = np.arange(len(full_values))
    else:
        parts = []
        # pre
        parts.append(pd.Series(pre_dense_ts) if pre_dense_ts is not None else pd.Series(np.arange(len(pre_dense))))
        # ft
        if len(ft_dense) > 0:
            parts.append(pd.Series(ft_dense_ts) if ft_dense_ts is not None else pd.Series(np.arange(len(ft_dense))))
        # val
        if len(val_raw) > 0:
            parts.append(pd.Series(val_ts) if val_ts is not None else pd.Series(np.arange(len(val_raw))))
        full_ts = pd.concat(parts, ignore_index=True).values

    ts_for_pred = full_ts[seq_len:]
    input_data  = full_values[seq_len-1 : -1]
    actual      = y_full_inv
    predicted   = y_full_hat_inv

    # 8) ì €ì¥(ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬/ì»¨í”¼ê·¸/CSV) â†’ re_saved_model/
    os.makedirs(MODEL_DIR, exist_ok=True)
    model_path  = os.path.join(MODEL_DIR, f"{artifacts_prefix}_model.keras")
    scaler_path = os.path.join(MODEL_DIR, f"{artifacts_prefix}_scaler.pkl")
    config_path = os.path.join(MODEL_DIR, f"{artifacts_prefix}_config.json")
    csv_path    = os.path.join(MODEL_DIR, f"{artifacts_prefix}_full_predictions.csv")

    # ìƒˆ ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    model.save(model_path)
    with open(scaler_path, "wb") as f:
        pickle.dump(scaler, f)

    # config ì €ì¥ (base ì—¬ë¶€/seq_len ê¸°ë¡)
    cfg = {
        "base_used": base_model is not None,
        "seq_len": int(seq_len),
        "densify_factor": int(DENSIFY_FACTOR),
        "splits": {"pre": SPLIT_PRE, "finetune": SPLIT_FT, "val": 1.0 - SPLIT_FT},
        "columns": {"target": value_col},
        "source_csv": csv_path.replace("_full_predictions.csv", "_SOURCE.csv"),  # ì˜ë¯¸ í‘œì‹œìš© í…ìŠ¤íŠ¸
        "notes": "full_predictions.csv = ì „ì²´ êµ¬ê°„ 1-step ì˜ˆì¸¡; timestamp,input_data,actual,predicted",
    }
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(cfg, f, ensure_ascii=False, indent=2)

    # ì˜ˆì¸¡ CSV ì €ì¥ (timestamp,input_data,actual,predicted)
    out_df = pd.DataFrame({
        "timestamp": ts_for_pred,
        "input_data": input_data,
        "actual": actual,
        "predicted": predicted
    })
    if np.issubdtype(out_df["timestamp"].dtype, np.datetime64):
        out_df["timestamp"] = pd.to_datetime(out_df["timestamp"])
    out_df.to_csv(csv_path, index=False)

    print(f"[{artifacts_prefix}] ì €ì¥ ì™„ë£Œ â†’")
    print(f"  Base used : {base_model is not None}")
    print(f"  Model     : {model_path}")
    print(f"  Scaler    : {scaler_path}")
    print(f"  Config    : {config_path}")
    print(f"  CSV       : {csv_path}")

    # ê°„ë‹¨ ì§€í‘œ(ì „ì²´ êµ¬ê°„)
    r = {
        "sensor": artifacts_prefix,
        "used_base": bool(base_model is not None),
        "seq_len": int(seq_len),
        "rows_csv": len(out_df),
        "csv": csv_path
    }
    print(f"[{artifacts_prefix}] summary: {r}")
    return r

# =============================
# ë©”ì¸
# =============================
if __name__ == "__main__":
    os.makedirs(MODEL_DIR, exist_ok=True)

    print("=" * 90)
    print("ğŸŒ¡ ê³µì • ì„¼ì„œ ì¬í•™ìŠµ (BASE ëª¨ë¸ warm-start) â†’ ì „ì²´ ì˜ˆì¸¡ CSV ì €ì¥")
    print("=" * 90)

    summary = []
    for s in SENSORS:
        res = retrain_one_sensor(
            csv_path=s["csv_path"],
            value_col=s["column"],          # 'actual'
            artifacts_prefix=s["artifacts_prefix"]
        )
        summary.append(res)

    summary_df = pd.DataFrame(summary)
    summary_path = os.path.join(MODEL_DIR, "retrain_summary.csv")
    summary_df.to_csv(summary_path, index=False)

    print("\n" + "=" * 90)
    print("âœ… ëª¨ë“  ì„¼ì„œ ì¬í•™ìŠµ ì™„ë£Œ (warm-start) & ì „ì²´ ì˜ˆì¸¡ CSV ì €ì¥")
    print("=" * 90)
    print(f"ğŸ“Š ìš”ì•½ CSV: {summary_path}")
    print(summary_df)
